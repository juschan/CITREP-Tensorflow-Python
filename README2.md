# Notes on Tensorflow

### Resources

Tertiary Course materials [here](https://github.com/tertiarycourses/CITREP-Tensorflow-Python).

### Configuration

1. Clone or download code.
2. Upload to Google Drive > "Colab Notebooks"
3. Open in Google Colab
4. Go to Runtime > Change Runtime Type > Hardware Accelerator > GPU

### Tensors

[WTF is a Tensor?!?](https://www.kdnuggets.com/2018/05/wtf-tensor.html).

Note that rank of a tensor is somewhat different to rank of matrix.


### Keras Input
[Keras Input Shape](https://stackoverflow.com/questions/44747343/keras-input-explanation-input-shape-units-batch-size-dim-etc)


### Classification
[Categorical Cross Entropy - Softmax, Cross Entorpy Loss](https://gombru.github.io/2018/05/23/cross_entropy_loss/)

[Sparse Categorical Entropy](https://jovianlin.io/cat-crossentropy-vs-sparse-cat-crossentropy/)
Sparse CE - label encoding
normal CE - one hot encoding

### CNN

[Max Pooling Layer](https://www.google.com/search?q=what+is+a+max+pooling+layer&client=firefox-b-d&sxsrf=ALeKk00LiduRNdZZtcXcB0sKN136I1ztZQ:1585152342155&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjGj5PSgLboAhXCXisKHRm3AvEQ_AUoAXoECA0QAw&biw=1680&bih=936#imgrc=fVc385NgNaXtYM)

[Convolution layer - viz!](http://cs231n.github.io/convolutional-networks/#conv)

[Convolution image example](https://setosa.io/ev/image-kernels/)

[CNN Architecture Calcs](https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-ii-hyper-parameter-42efca01e5d7)

[CNN - Feature Extraction vs Classification](https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/b5d1f29a-060d-43be-b4a6-d12f79a8c844/9fc8b854-4528-44d9-be5f-792c9119ee90/images/screenshot.PNG)

### Tensorboard

[Setup Tensorboard on Google Colab](https://medium.com/@kuanhoong/how-to-use-tensorboard-with-google-colab-43f7cf061fe4)


### RNN

[Beginner guide to RNN](https://medium.com/@camrongodbout/recurrent-neural-networks-for-beginners-7aca4e933b82)

[Illustrated Guide to LSTM, GRU](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)

[RNN by Example](https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470)

[Vanishing Gradient Problem, and how Batch Normalisation helps solve it](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)

[bidirectional lstm](https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks)

----

[Good video - 1](https://www.youtube.com/watch?v=UNmqTiOnRfg)

[Good Video - 2](https://www.youtube.com/watch?v=WCUNPb-5EYI)

[Explanation -1 ](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)

[LSTM - explanation](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

[RNN (various) - illustrated](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)

### Tensorhub

[Mobilenet v2, Classification](https://tfhub.dev/google/imagenet/mobilenet_v2_035_160/classification/4)





